# -*- coding: utf-8 -*-
"""Copia de ids330L_04_aprendizaje-maquina.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1oVU7sQEPzUueMBTHyEI6eJs9odLH7VYl

# Aprendizaje de Máquina

## 1. Regresión Lineal

La regresión lineal es un método utilizado en estadística para entender la relación entre dos variables. Imagina que tienes dos conjuntos de datos, uno se llama variable independiente (X) y otro se llama variable dependiente (Y). La regresión lineal busca encontrar una línea recta que mejor se ajuste a esos datos.

Esta línea recta se llama "línea de regresión" y se representa mediante una ecuación de la forma Y = aX + b. Aquí, "a" representa la pendiente de la línea, que indica cuánto cambia la variable dependiente (Y) cuando la variable independiente (X) aumenta en una unidad. "b" es el punto de intersección de la línea con el eje Y, y muestra el valor de Y cuando X es igual a cero.

El objetivo de la regresión lineal es encontrar los valores de "a" y "b" que minimicen la distancia entre los puntos de datos y la línea de regresión. Esto nos permite predecir los valores de Y para nuevos valores de X basándonos en la relación lineal encontrada.

En esta práctica vamos a utilizar el algoritmo de regresión lineal para encontrar la relación entre dos variables. Para ello tenemos que:

1. Seleccionar los datos.
2. Definir nuestro modelo.
3. Entrenar el modelo en base a los datos.
4. Validar nuestro modelo entrenado.

---

Pregunta: ¿Esto solo funciona para valores numericos?

Respuesta: Se debe convertir el valor no numerico a numerico:

```
Valores categoricos a numerico
Forma 1
usa -> 0
mexico -> 1
RD -> 2

Forma 2 (one hot encoding)
usa -> [0, 0, 1]
mexico -> [0, 1, 0]
RD -> [1, 0, 0]

Texto a numero
1. Sacar metricas estadisticas del texto (e.g., frecuencia de alguna palabra, cantidad de veces que se repite una palabra, etc.)

"El año pasado me pase la navidad en familia. Me gusta la navidad, pero he engordado ese año"

=> {abanico: 0, navida: 2, familia: 1, año: 2}
=> [0, 2, 0, ..., 1, ..., 2]

2. Redes Neuronales (word2vec)
"El año pasado me pase la navidad en familia. Me gusta la navidad, pero mi padre y yo hemos engordado ese año"

"año" -> red neuronal -> [0.2, 0.4, ..., 0.6]
"familia" -> red neuronal -> [0.5, 0.8, ..., 0.1]
"padre" -> red neuronal -> [0.6, 0.8, ..., 0.2]

```
"""

import numpy as np
import pandas as pd
import seaborn as sn
import matplotlib.pyplot as plt

data = sn.load_dataset('mpg')
data.head()

target = 'mpg'

data.corr()

# randomizar el orden de la data
data = data.sample(n=len(data))

n = len(data)
train_n = int(n * 0.8)  # 80%
train_data = data[:train_n]
test_data = data[train_n:]

attribute = 'weight'
target = 'mpg'

x_train = train_data[attribute].values
y_train = train_data[target].values

x_test = test_data[attribute].values
y_test = test_data[target].values

plt.scatter(x_train,y_train)
plt.scatter(x_test, y_test)
plt.xlabel(attribute)
plt.ylabel(target)
plt.show()

"""La "línea de regresión" se representa mediante una ecuación de la forma $Y = θ_0 + \theta_1 x_1 + \theta_2 x_2 + ... + \theta_n x_n$ o $Y = Θ \cdot X$.

En inteligencia artificial, una hipótesis es una suposición o conjetura sobre una relación entre variables o características que se utiliza para formular un modelo o algoritmo. En el contexto del aprendizaje automático, una hipótesis se refiere a una función o modelo matemático propuesto que intenta capturar la relación subyacente entre las variables de entrada y salida.

Entonces, dada nuestra data, la hipótesis está definida por la siguiente ecuación:

$h = \theta_1 x + \theta_0$
"""

# Modelo
theta_0 = np.random.random()
theta_1 = np.random.random()

h = theta_0 + theta_1 * x_train

plt.scatter(x_train, y_train)
plt.plot(x_train, h, 'red')
plt.show()

# Funcion de error
def mse(y, h):
    # sum = []
    # n = len(y)
    # for i in range(n):
    #     sum += (y[i] - h[i])**2
    # return sum/n
    return ((y - h)**2).mean()

"""```
d_theta_0 = - 2 * (y - h).sum()
d_theta_1 = - 2 * (y - h).dot(x)
```
"""

# Proceso de entrenamiento

learning_rate = 0.0001
epochs = 1000

attribute = 'weight'
target = 'mpg'

x_train = train_data[attribute].values
x_train = (x_train - x_train.min())/(x_train.max() - x_train.min())
y_train = train_data[target].values

x_test = test_data[attribute].values
y_test = test_data[target].values

error_list = []
theta_0 = np.random.random()
theta_1 = np.random.random()
for epoch in range(0, epochs):
    h = theta_0 + theta_1 * x_train
    error = mse(y_train, h)
    error_list.append(error)

    if epoch%100==0:
        print(error)

    d_theta_0 = - 2 * (y_train - h).sum()
    d_theta_1 = - 2 * (y_train - h).dot(x_train)

    # modificando parametros
    theta_0 = theta_0 - learning_rate * d_theta_0
    theta_1 = theta_1 - learning_rate * d_theta_1

print(f"El model es h = {theta_1}*x + {theta_0}")

h = theta_0 + theta_1 * x_train

plt.scatter(x_train, y_train)
plt.plot(x_train, h, 'red')
plt.show()

# Evaluar el modelo
x_test = (x_test - x_test.min())/(x_test.max() - x_test.min())
h = theta_0 + theta_1 * x_test

plt.scatter(x_test, y_test)
plt.plot(x_test, h, 'red')
plt.show()

x_new = 3000
x_new_norm = (3000 - 1500)/(5000 - 1500)
pred = theta_0 + theta_1 * x_new_norm
print(f"Weight: {x_new}. Normalizado: {x_new_norm}. MPG: {pred}")

"""## 2. Regresión Logistica => Parte 1

Objetivo: clasificar las observaciones como "setosa" o "no setosa"
"""

data = sn.load_dataset('iris')
data.head()

sn.scatterplot(data=data, x='sepal_length', y='sepal_width', hue='species')

data['label'] = [1 if specie == 'setosa' else 0 for specie in data['species']]
sn.scatterplot(data=data, x='sepal_length', y='sepal_width', hue='label')

"""### 2.1. Elegir 2 atributos (e.g., sepal_width, sepal_length)"""

features = ['sepal_width', ' sepal_length ']
target = 'label'

"""### 2.2. Dividir la data en training / testing"""

import seaborn as sns
from sklearn.model_selection import train_test_split

data = sns.load_dataset('iris')

features = ['sepal_width', 'sepal_length']
target = 'species'

x_train, x_test, y_train, y_test = train_test_split(data[features], data[target], test_size=0.3, random_state=42)

"""### 2.3. Definir el modelo

$h = \frac{1}{1 + e^{-z}}$

$z = \theta_0 + \theta_1 \cdot x_1 + \theta_2 \cdot x_2$
"""

from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression

iris = load_iris()
X = iris.data
y = iris.target

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

log_reg = LogisticRegression()

log_reg.fit(X_train, y_train)

"""### 2.4. Función de error (entropia cruzada)"""

import numpy as np

def sigmoid(z):
    return 1 / (1 + np.exp(-z))

def cross_entropy_loss(y_true, y_pred):
    m = y_true.shape[0]
    loss = -(1/m) * np.sum(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))
    return loss

y_true = np.array([1, 0, 1, 1, 0])
y_pred = sigmoid(np.array([0.9, -0.1, 0.8, 0.7, -0.2]))

loss = cross_entropy_loss(y_true, y_pred)
print(f"Entropía cruzada: {loss}")

"""### 2.5. Entrenar
* No tienen que normalizar la data
"""

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.datasets import load_iris
import numpy as np

iris = load_iris()
X = iris.data[:, :2]

y = (iris.target == 0).astype(int)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

model = LogisticRegression()

model.fit(X_train, y_train)

print("Model trained.")

"""### 2.6. Evaluar el modelo

m = -(theta_1 / theta_2)
b = -(theta_0 / theta_2)

sns.scatterplot(x=x_test[:,0], y=x_test[:,1], hue=y_test)
ax = plt.gca()
x_values = np.array(ax.get_xlim())
y_values = b + m * x_values
plt.plot(x_values, y_values, ls='--', c='k')
plt.fill_between(x_values, y_values, 2, color='tab:orange', alpha=0.2)
plt.fill_between(x_values, y_values, 4.5, color='tab:blue', alpha=0.2)
plt.xlim(x_values[0], x_values[1])
plt.ylim(2, 4.5)
plt.xlabel(attributes[0])
plt.ylabel(attributes[1])
plt.show()

## 3. Aprendizaje no Supervisado: K-means
"""

from sklearn import datasets

x, y = datasets.make_blobs(random_state=1)
sn.scatterplot(x=x[:,0], y=x[:,1], hue=y)
plt.show()

sn.scatterplot(x=x[:,0], y=x[:,1])
plt.xlabel("1er componente")
plt.ylabel("2do componente")
plt.show()

def initialize_centroids(k, x):
    m, n = x.shape
    centroids = np.empty((k,n))
    for i in range(k):
        centroids[i] = x[np.random.choice(range(m))]
    return centroids

def euclidean_distance(x1, x2):
    return np.sqrt(np.sum((x1-x2)**2))

def closest_centroid(x, centroids, k):
    distances = np.empty(k)
    for i in range(k):
        distances[i] = euclidean_distance(centroids[i], x)
    return np.argmin(distances)

def create_clusters(x, centroids, k):
    m, n = np.shape(x)
    cluster_idx = np.empty(m)  # una lista de m numeros porque hay m cantidad de datos
    for i in range(m):
        cluster_idx[i] = closest_centroid(x[i], centroids, k)
    return cluster_idx

def compute_means(cluster_idx, x, k):
    m, n = np.shape(x)
    centroids = np.empty((k, n))
    for i in range(k):
        points = x[cluster_idx==i]
        centroids[i] = np.mean(points, axis=0)
    return centroids

k = 2 # [abnormal, normal]
epochs = 10

centroids = initialize_centroids(k=k, x=x)

for epoch in range(epochs):

    clusters = create_clusters(centroids=centroids, k=k, x=x)
    previous_centroids = centroids
    centroids = compute_means(cluster_idx=clusters, k=k, x=x)


    sn.scatterplot(x=x[:,0], y=x[:,1], hue=clusters)
    for c in centroids:
        plt.plot(c[0], c[1], 'ro')
    plt.xlabel("1er componente")
    plt.ylabel("2do componente")
    plt.show()

    diff = previous_centroids - centroids
    if not diff.any():
        break

"""## 4. Arbol de Decision (Regresión)"""

from sklearn import tree
from sklearn.model_selection import train_test_split

data = sn.load_dataset('mpg')
print(data.shape)
data = data.dropna()
print(data.shape)
data.head()

all_features = ['cylinders', 'displacement', 'horsepower', 'weight', 'acceleration']
target = 'mpg'
x = data[all_features].values
y = data[target].values

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=1)

# dt: decision_tree
dt = tree.DecisionTreeRegressor(random_state=1, max_depth=3)
dt = dt.fit(x_train, y_train)

y_pred = dt.predict(x_test)
mse(y_test, y_pred)

def tree_to_rules(dt, all_features):
  tree_ = dt.tree_
  feature_names = [
    all_features[i] if i != tree._tree.TREE_UNDEFINED else 'undefined'
    for i in tree_.feature
  ]

  def recurse(node, depth):
    indent = "- " * depth
    if tree_.feature[node] != tree._tree.TREE_UNDEFINED:
      name = feature_names[node]
      threshold = tree_.threshold[node]
      print(f"{indent}if {name} <= {threshold:.2f}:")
      recurse(tree_.children_left[node], depth + 1)
      print(f"{indent}else:  # if {name} > {threshold:.2f}")
      recurse(tree_.children_right[node], depth + 1)
    else:
      print(f"{indent}return >>> {tree_.value[node]}")

  recurse(0, 0)

tree_to_rules(dt, all_features)

"""## 5. Arbol de Decision (Clasificación) -> Tarea parte 2

Utilizando un arbol de decision (scikit-learn), desarrollen un modelo para predecir la especie de la flor basado en [sepal_length, sepal_width, petal_length, petal_width]
"""

data = sn.load_dataset('iris')
data = data.dropna()
data.head()

from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

iris = load_iris()
X = iris.data
y = iris.target

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

decision_tree = DecisionTreeClassifier()

decision_tree.fit(X_train, y_train)

y_pred = decision_tree.predict(X_test)

accuracy = accuracy_score(y_test, y_pred)

print(f"Accuracy: {accuracy}")